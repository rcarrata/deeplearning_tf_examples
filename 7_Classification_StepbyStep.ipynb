{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_Classification_StepbyStep.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/xqjjQHxlYkLrCCuNtCud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcarrata/deeplearning_tf_examples/blob/master/7_Classification_StepbyStep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 0 - Classification Definition\n",
        "\n",
        "Classification is used to predict a discrete label. The outputs fall under a finite set of possible outcomes. Many situations have only two possible outcomes. This is called binary classification (True/False, 0 or 1, Hotdog / not Hotdog).\n",
        "\n",
        "For example:\n",
        "\n",
        "* Predict whether an email is spam or not\n",
        "* Predict whether it will rain or not\n",
        "* Predict whether a user is a power user or a casual user\n",
        "\n",
        "There are also two other common types of classification: **multi-class classification** and **multi-label classification**.\n",
        "\n",
        "**Multi-class classification** has the same idea behind binary classification, except instead of **two possible outcomes**, there are three or more.\n",
        "\n",
        "For example:\n",
        "\n",
        "* Predict whether a photo contains a pear, apple, or peach\n",
        "* Predict what letter of the alphabet a handwritten character is\n",
        "* Predict whether a piece of fruit is small, medium, or large\n",
        "\n",
        "An important note about binary and multi-class classification is that in both, each outcome has one specific label. However, in multi-label classification, there are multiple possible labels for each outcome. This is useful for customer segmentation, image categorization, and sentiment analysis for understanding text. To perform these classifications, we use models like **Naive Bayes, K-Nearest Neighbors, SVMs**, as well as various deep learning models."
      ],
      "metadata": {
        "id": "62kKGIRQbj2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 1 - Cross-entropy\n",
        "\n",
        "Before we continue loading the data and designing the model, we need to talk about cross-entropy, an important concept for evaluating classification model training. \n",
        "\n",
        "* **Cross-entropy** is a **score that summarizes the average difference between the actual and predicted probability distributions for all classes**. \n",
        "\n",
        "---\n",
        "* **The goal** is to **minimize the score**, with a ***perfect cross-entropy value is 0***.\n",
        "---\n",
        "\n",
        "For example, consider a problem with three classes, each having three examples in the data classified in class 1, class 2, and class 3, respectively. They are represented with one-hot encoding.\n",
        "\n",
        "* Let the true distribution for each example be:\n",
        "\n",
        "```python\n",
        "#the first class is set to probability 1, all others are 0; this example belongs to class #1\n",
        "ex_1_true = [1, 0, 0] \n",
        "#the second class is set to probability 1, all others are 0;this example belongs to class #2\n",
        "ex_2_true = [0, 1, 0] \n",
        "#the third class is set to probability 1, all others are 0;this example belongs to class #3\n",
        "ex_3_true = [0, 0, 1]\n",
        "```\n",
        "\n",
        "Now imagine a predictive model that gave us the following predictions:\n",
        "\n",
        "```python\n",
        "#the highest probability is given to class #1\n",
        "ex_1_predicted = [0.7, 0.2, 0.1] \n",
        "#the highest probability is given to class #2\n",
        "ex_2_predicted = [0.1, 0.8, 0.1] \n",
        "#the highest probability is given to class #3\n",
        "ex_3_predicted = [0.2, 0.2, 0.6] \n",
        "```\n",
        "\n",
        "If we compare the true and predicted distributions above, they seem to be rather different numbers, but there is a good pattern here: each example’s predicted distribution gives the highest probability to the label the example actually belongs to. This means the distributions are similar and the cross-entropy should be small. When we calculate cross-entropy for the example above, we get 0.364, which is rather good and close to 0.\n",
        "\n",
        "* Now, consider a bad predictive model that gives the highest probability to a wrong label every time:\n",
        "\n",
        "```python\n",
        "#the highest probability given to class #3, true labels is class #1\n",
        "ex_1_predicted_bad = [0.1, 0.1, 0.7]\n",
        "#the highest probability given to class #1, true labels is class #2\n",
        "ex_2_predicted_bad = [0.8, 0.1, 0.1] \n",
        "#the highest probability given to class #1, true labels is class #3\n",
        "ex_3_predicted_bad = [0.6, 0.2, 0.2]\n",
        "```\n",
        "\n",
        "When we calculate the cross-entropy for these examples, we get 2.036, which is rather bad.\n",
        "\n",
        "If we take cross-entropy between two identical true distributions, we get perfect probabilities and cross-entropy equal to 0.\n",
        "\n",
        "Run the code on the right to see this in practice. To calculate cross-entropy between two distributions we are using the log_loss() function in scikit-learn, which is equivalent to calculating cross-entropy."
      ],
      "metadata": {
        "id": "0dRO1KTP1fOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp37I9PZ0-Da",
        "outputId": "48f18f41-7a64-4838-f82e-d414261acef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Log Loss (good prediction): 0.364\n",
            "Average Log Loss (bad prediction): 2.036\n",
            "(TODO)Average Log Loss (true prediction): 2.036\n"
          ]
        }
      ],
      "source": [
        "## Exercise 1 - Cross-Entropy\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "#the first class is set to probability 1, all others are 0; this example belongs to class #1\n",
        "ex_1_true = [1, 0, 0] \n",
        "#the second class is set to probability 1, all others are 0;this example belongs to class #2\n",
        "ex_2_true = [0, 1, 0] \n",
        "#the third class is set to probability 1, all others are 0;this example belongs to class #3\n",
        "ex_3_true = [0, 0, 1] \n",
        "\n",
        "#the highest probability is given to class #1\n",
        "ex_1_predicted = [0.7, 0.2, 0.1] \n",
        "#the highest probability is given to class #2\n",
        "ex_2_predicted = [0.1, 0.8, 0.1] \n",
        "#the highest probability is given to class #3\n",
        "ex_3_predicted = [0.2, 0.2, 0.6] \n",
        "\n",
        "#the highest probability given to class #3, true labels is class #1\n",
        "ex_1_predicted_bad = [0.1, 0.1, 0.7]\n",
        "#the highest probability given to class #1, true labels is class #2\n",
        "ex_2_predicted_bad = [0.8, 0.1, 0.1] \n",
        "#the highest probability given to class #1, true labels is class #3\n",
        "ex_3_predicted_bad = [0.6, 0.2, 0.2] \n",
        "\n",
        "true_labels = [ex_1_true, ex_2_true, ex_3_true]\n",
        "predicted_labels = [ex_1_predicted, ex_2_predicted, ex_3_predicted]\n",
        "predicted_labels_bad = [ex_1_predicted_bad, ex_2_predicted_bad, ex_3_predicted_bad]\n",
        "\n",
        "ll = log_loss(true_labels, predicted_labels)\n",
        "print('Average Log Loss (good prediction): %.3f' % ll)\n",
        "\n",
        "ll = log_loss(true_labels, predicted_labels_bad)\n",
        "print('Average Log Loss (bad prediction): %.3f' % ll)\n",
        "\n",
        "#your code here\n",
        "print('(TODO)Average Log Loss (true prediction): %.3f' % ll)\n",
        "ll = log_loss(true_labels, true_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 2 - Loading and analyzing the data\n",
        "\n",
        "Assume we have a dataset, stored in the train_glass.csv (training data) and test_glass.csv (test data) files, about various products made of glass. \n",
        "\n",
        "Using the train_glass.csv file, we want to learn a model that can **predict which glass item can be constructed** given the proportion of **various elements such as Aluminium (Al), Magnesium (Mg), and Iron (Fe)**. We then want to evaluate the model on the test data.\n",
        "\n",
        "* To load the training data into a pandas DataFrame, we do the following:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "data_train = pd.read_csv(\"train_glass.csv\")\n",
        "```\n",
        "\n",
        "* The following command lists all features with accompanying types about the columns:\n",
        "\n",
        "```python\n",
        "print(data_train.info())\n",
        "```\n",
        "\n",
        "* The output looks something like this:\n",
        "\n",
        "```bash\n",
        "#   Column    Non-Null Count   Dtype  \n",
        "---  ------   --------------   -----  \n",
        " 0   Al       300 non-null     float64\n",
        " 1   Mg       300 non-null     float64 \n",
        " 3   Fe       300 non-null     float64\n",
        " 4   item     300 non-null     object\n",
        "```\n",
        "\n",
        "\n",
        "We see that Al, Mg, and Fe are numeric columns, and item is an object column containing strings. We would like to predict the item column.\n",
        "\n",
        "* The following commands show us which categories we have in the item column and what their distribution is:\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "print('Classes and number of values in the dataset`,Counter(data_train[“item”]))\n",
        "```\n",
        "\n",
        "which gives something like the following output:\n",
        "\n",
        "```python\n",
        "{‘lamps’: 75, ‘tableware’: 125, 'containers': 100}\n",
        "```\n",
        "\n",
        "This tells us that we have three categories to predict: “lamps”, “tableware”, and “containers”, and how many samples we have in our training data for each.\n",
        "\n",
        "Next, we we need to split our data into features and labels by doing the following:\n",
        "\n",
        "```python\n",
        "train_x = data_train[\"item\"]\n",
        "train_y = data_train[[‘Al', ‘Mg’, 'Fe’]]\n",
        "```"
      ],
      "metadata": {
        "id": "b3TnSgozWQU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISES 2 - Loading and analyzing the data\n",
        "\n",
        "1. Using pandas, load the air_quality_train.csv into a DataFrame instance called train_data, and load the air_quality_test.csv into a DataFrame instance called test_data.\n",
        "\n",
        "2. Using DataFrame.info() print all columns with their respective types in the train_data DataFrame.\n",
        "\n",
        "3. Using collections.Counter() to print the class distribution for the Air_Quality column in the train_data DataFrame.\n",
        "\n",
        "4. Extract the features columns from train_data DataFrame where feature columns are ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI'], and assign the result to x_train.\n",
        "\n",
        "5. Extract the label column “Air_Quality” from the train_data DataFrame, and assign the result to y_train."
      ],
      "metadata": {
        "id": "P_jmBvFMW7da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/Colab/Classification/air_quality_train.csv\"\n",
        "!ls \"/content/drive/My Drive/Colab/Classification/air_quality_test.csv\"\n",
        "\n",
        "root_folder = \"/content/drive/My Drive/Colab/\"\n",
        "project_folder = \"Classification/\"\n",
        "csv_file_1 = \"air_quality_train.csv\"\n",
        "csv_file_2 = \"air_quality_test.csv\"\n",
        "\n",
        "csv_data_1 = root_folder + project_folder + csv_file_1\n",
        "print(csv_data_1)\n",
        "\n",
        "csv_data_2 = root_folder + project_folder + csv_file_2\n",
        "print(csv_data_2)\n",
        "\n",
        "train_data = pd.read_csv(csv_data_1)\n",
        "test_data = pd.read_csv(csv_data_2)\n",
        "\n",
        "from google.colab.data_table import DataTable\n",
        "DataTable.max_columns = 60\n",
        "\n",
        "#print the class distribution\n",
        "print(train_data.info())\n",
        "print(test_data.info())\n",
        "\n",
        "#extract the features from the training data\n",
        "print('Air_Quality',Counter(train_data[\"Air_Quality\"]))\n",
        "print('Air_Quality',Counter(test_data[\"Air_Quality\"]))\n",
        "\n",
        "#extract the label column from the training data\n",
        "x_train = train_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "y_train = train_data[\"Air_Quality\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssmCdAuhYnBI",
        "outputId": "adf626a0-24cc-49b4-872d-103cf3404111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'/content/drive/My Drive/Colab/Classification/air_quality_train.csv'\n",
            "'/content/drive/My Drive/Colab/Classification/air_quality_test.csv'\n",
            "/content/drive/My Drive/Colab/Classification/air_quality_train.csv\n",
            "/content/drive/My Drive/Colab/Classification/air_quality_test.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7782 entries, 0 to 7781\n",
            "Data columns (total 14 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PM2.5        7782 non-null   float64\n",
            " 1   PM10         7782 non-null   float64\n",
            " 2   NO           7782 non-null   float64\n",
            " 3   NO2          7782 non-null   float64\n",
            " 4   NOx          7782 non-null   float64\n",
            " 5   NH3          7782 non-null   float64\n",
            " 6   CO           7782 non-null   float64\n",
            " 7   SO2          7782 non-null   float64\n",
            " 8   O3           7782 non-null   float64\n",
            " 9   Benzene      7782 non-null   float64\n",
            " 10  Toluene      7782 non-null   float64\n",
            " 11  Xylene       7782 non-null   float64\n",
            " 12  AQI          7782 non-null   float64\n",
            " 13  Air_Quality  7782 non-null   object \n",
            "dtypes: float64(13), object(1)\n",
            "memory usage: 851.3+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1394 entries, 0 to 1393\n",
            "Data columns (total 14 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PM2.5        1394 non-null   float64\n",
            " 1   PM10         1394 non-null   float64\n",
            " 2   NO           1394 non-null   float64\n",
            " 3   NO2          1394 non-null   float64\n",
            " 4   NOx          1394 non-null   float64\n",
            " 5   NH3          1394 non-null   float64\n",
            " 6   CO           1394 non-null   float64\n",
            " 7   SO2          1394 non-null   float64\n",
            " 8   O3           1394 non-null   float64\n",
            " 9   Benzene      1394 non-null   float64\n",
            " 10  Toluene      1394 non-null   float64\n",
            " 11  Xylene       1394 non-null   float64\n",
            " 12  AQI          1394 non-null   float64\n",
            " 13  Air_Quality  1394 non-null   object \n",
            "dtypes: float64(13), object(1)\n",
            "memory usage: 152.6+ KB\n",
            "None\n",
            "Air_Quality Counter({'Very Poor': 1297, 'Poor': 1297, 'Moderate': 1297, 'Satisfactory': 1297, 'Severe': 1297, 'Good': 1297})\n",
            "Air_Quality Counter({'Moderate': 508, 'Satisfactory': 452, 'Poor': 172, 'Very Poor': 125, 'Good': 100, 'Severe': 37})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 3 - Preparing the data\n",
        "\n",
        "When using categorical cross-entropy — the loss function necessary for multiclass classification problems — in TensorFlow with Keras, **one needs to convert all the categorical features and labels into one-hot encoding vectors**. \n",
        "\n",
        "Previously, when we had features encoded as strings, we used the pandas.get_dummies() function. This works well for features, but it’s **not very usable for labels**. The problem is that **get_dummies() creates a separate column for each category, and you cannot predict for multiple columns**.\n",
        "\n",
        "* A **better approach** is to ***convert the label vectors to integers ranging from 0 to the number of classes by using sklearn.preprocessing.LabelEncoder***:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "train_y=le.fit_transform(train_y.astype(str))\n",
        "test_y=le.transform(test_y.astype(str))\n",
        "```\n",
        "\n",
        "* We **first** ***fit the transformer to the training data using the LabelEncoder.fit_transform() method***, \n",
        "\n",
        "* and **then** ***fit the trained transformer to the test data using the LabelEncoder.transform() method***.\n",
        "\n",
        "* We can print the resulting mappings with:\n",
        "\n",
        "```python\n",
        "integer_mapping = {l: i for i, l in enumerate(le.classes_)}\n",
        "print(integer_mapping)\n",
        "```\n",
        "\n",
        "* We get the following output:\n",
        "\n",
        "```python\n",
        "{‘lamps’: 0, ‘tableware': 1, 'containers': 2}. \n",
        "```\n",
        "\n",
        "* Each category is mapped to an integer, from 0 to 2 (because we have three categories).\n",
        "\n",
        "* Now that we have labels as integers, we can use a **Keras function called to_categorical() to convert them into one-hot-encodings** — the format we need for our cross-entropy loss:\n",
        "\n",
        "```python\n",
        "train_y = tensorflow.keras.utils.to_categorical(train_y, dtype = ‘int64’)\n",
        "test_y = tensorflow.keras.utils.to_categorical(test_y, dtype = ‘int64’)\n",
        "```"
      ],
      "metadata": {
        "id": "U6KKy2rTagQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 3 - Preparing the Data\n",
        "\n",
        "1. Use the LabelEncoder.fit_transform() method to encode the label vector y_train into integers and assign the result back to the y_train variable.\n",
        "\n",
        "2. Use the le.transform() method to encode the label vector y_test into integers, where le is the instance of LabelEncoder trained in the previous step, and assign the result back to y_test.\n",
        "\n",
        "3. Using the tensorflow.keras.utils.to_categorical() function, convert the integer encoded label vector y_train into a one-hot encoding vector and assign the result back into the y_train variable.\n",
        "\n",
        "4. Using the tensorflow.keras.utils.to_categorical() function, convert the integer encoded label vector y_test into a one-hot encoding vector and assign the result back into the y_test variable."
      ],
      "metadata": {
        "id": "pMrqGmoMiEyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow\n",
        "#your code here\n",
        "\n",
        "#train_data = pd.read_csv(\"air_quality_train.csv\")\n",
        "#test_data = pd.read_csv(\"air_quality_test.csv\")\n",
        "\n",
        "#print columns and their respective types\n",
        "print(train_data.info())\n",
        "#print the class distribution\n",
        "print(Counter(train_data[\"Air_Quality\"]))\n",
        "#extract the features from the training data\n",
        "x_train = train_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the training data\n",
        "y_train = train_data[\"Air_Quality\"]\n",
        "#extract the features from the test data\n",
        "x_test = test_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the test data\n",
        "y_test = test_data[\"Air_Quality\"]\n",
        "\n",
        "#encode the labels into integers\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_train = le.fit_transform(y_train.astype(str))\n",
        "y_test = le.transform(y_test.astype(str))\n",
        "\n",
        "#print the integer mappings\n",
        "integer_mapping = {l: i for i, l in enumerate(le.classes_)}\n",
        "print(\"The integer mapping:\\n\", integer_mapping)\n",
        "\n",
        "#convert the integer encoded labels into binary vectors\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, dtype = 'int64')\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, dtype = 'int64')\n",
        "\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI8AC_x5h96a",
        "outputId": "be4f9fa8-4748-4969-ad7e-29d99a355737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7782 entries, 0 to 7781\n",
            "Data columns (total 14 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PM2.5        7782 non-null   float64\n",
            " 1   PM10         7782 non-null   float64\n",
            " 2   NO           7782 non-null   float64\n",
            " 3   NO2          7782 non-null   float64\n",
            " 4   NOx          7782 non-null   float64\n",
            " 5   NH3          7782 non-null   float64\n",
            " 6   CO           7782 non-null   float64\n",
            " 7   SO2          7782 non-null   float64\n",
            " 8   O3           7782 non-null   float64\n",
            " 9   Benzene      7782 non-null   float64\n",
            " 10  Toluene      7782 non-null   float64\n",
            " 11  Xylene       7782 non-null   float64\n",
            " 12  AQI          7782 non-null   float64\n",
            " 13  Air_Quality  7782 non-null   object \n",
            "dtypes: float64(13), object(1)\n",
            "memory usage: 851.3+ KB\n",
            "None\n",
            "Counter({'Very Poor': 1297, 'Poor': 1297, 'Moderate': 1297, 'Satisfactory': 1297, 'Severe': 1297, 'Good': 1297})\n",
            "The integer mapping:\n",
            " {'Good': 0, 'Moderate': 1, 'Poor': 2, 'Satisfactory': 3, 'Severe': 4, 'Very Poor': 5}\n",
            "[[0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " ...\n",
            " [0 0 0 0 0 1]\n",
            " [0 0 0 0 0 1]\n",
            " [0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 4 - Designing a deep learning model for classification\n",
        "\n",
        "To **initialize a Keras Sequential model in TensorFlow**, we do the following:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "my_model = Sequential()\n",
        "```\n",
        "\n",
        "The process is the following:\n",
        " * set the input layer\n",
        " * set the hidden layers\n",
        " * set the output layer.\n",
        "\n",
        "---\n",
        "To **add the input layer**, we use **keras.layers.InputLayer** the following way:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import  InputLayer\n",
        "my_model.add(InputLayer(input_shape=(data_train.shape[1],)))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "For now, we will only add one hidden layer using keras.layers.Dense:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import  Dense\n",
        "my_model.add(Dense(8, activation='relu'))\n",
        "```\n",
        "\n",
        "This layer has eight hidden units and uses a rectified linear unit (relu) as the activation function.\n",
        "\n",
        "---\n",
        "\n",
        "Finally, we need to set the **output layer**. \n",
        "\n",
        "* For regression, we don’t use any activation function in the final layer because we needed to predict a number without any transformations. \n",
        "\n",
        "* However, **for classification, the desired output is a vector of categorical probabilities**.\n",
        "\n",
        "\n",
        "---\n",
        "To have this vector as an output, we need to use the **softmax activation function** that ***outputs a vector with elements having values between 0 and 1 and that sum to 1*** (just as all the probabilities of all outcomes for a random variable must sum up to 1). \n",
        "\n",
        "* ***Softmax*** is a mathematical function that **converts a vector of numbers into a vector of probabilities**, where the probabilities of each value are proportional to the relative scale of each value in the vector.\n",
        "\n",
        "---\n",
        "In the case of a ***binary classification problem***, a **sigmoid activation function** can also be used in the **output layer but paired with the binary_crossentropy loss**.\n",
        "\n",
        "***Binary classification*** refers to those classification tasks that have two class labels.\n",
        "\n",
        "Examples include:\n",
        "\n",
        "* Email spam detection (spam or not).\n",
        "* Churn prediction (churn or not).\n",
        "* Conversion prediction (buy or not).\n",
        "---\n",
        "\n",
        "Since we have 3 classes to predict in our glass production data, the final softmax layer must have 3 units:\n",
        "\n",
        "```python\n",
        "my_model.add(Dense(3, activation='softmax')) #the output layer is a softmax with 3 units\n",
        "```"
      ],
      "metadata": {
        "id": "oaZaUc4gB7He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 4 - Designing a deep learning model for classification\n",
        "\n",
        "1. To your model declaration model add an input layer using tensorflow.keras.layers.InputLayer.\n",
        "\n",
        "2. To your model instance model add a hidden layer using tensorflow.keras.layers.Dense with 10 neurons and relu activation function.\n",
        "\n",
        "3. To your model, add an output layer as an instance of tensorflow.keras.layers.Dense with softmax as the activation function, and the number of hidden units corresponding to the number of classes in the air quality data."
      ],
      "metadata": {
        "id": "wymXhP5bEdlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import  InputLayer\n",
        "from tensorflow.keras.layers import  Dense\n",
        "#your code here\n",
        "\n",
        "#train_data = pd.read_csv(\"air_quality_train.csv\")\n",
        "#test_data = pd.read_csv(\"air_quality_test.csv\")\n",
        "\n",
        "#print columns and their respective types\n",
        "print(train_data.info())\n",
        "#print the class distribution\n",
        "print(Counter(train_data[\"Air_Quality\"]))\n",
        "#extract the features from the training data\n",
        "x_train = train_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the training data\n",
        "y_train = train_data[\"Air_Quality\"]\n",
        "#extract the features from the test data\n",
        "x_test = test_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the test data\n",
        "y_test = test_data[\"Air_Quality\"]\n",
        "\n",
        "#encode the labels into integers\n",
        "le = LabelEncoder()\n",
        "#convert the integer encoded labels into binary vectors\n",
        "y_train=le.fit_transform(y_train.astype(str))\n",
        "y_test=le.transform(y_test.astype(str))\n",
        "\n",
        "integer_mapping = {l: i for i, l in enumerate(le.classes_)}\n",
        "print(\"The integer mapping:\\n\", integer_mapping)\n",
        "\n",
        "#convert the integer encoded labels into binary vectors\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, dtype = 'int64')\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, dtype = 'int64')\n",
        "\n",
        "#design the model\n",
        "model = Sequential()\n",
        "\n",
        "#add the input layer\n",
        "model.add(InputLayer(input_shape=(x_train.shape[1],)))\n",
        "\n",
        "#add a hidden layer\n",
        "model.add(Dense(10, activation='relu'))\n",
        "\n",
        "#add an output layer\n",
        "model.add(Dense(6, activation='softmax')) # That is how many classes we have in the Air Quality data (6 in total). Check the integer mapping of the label \"Air Quality\" that we want to classify.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZU3Up2-EnbN",
        "outputId": "44bf800b-e37d-4fd9-d669-8470ee6e02fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7782 entries, 0 to 7781\n",
            "Data columns (total 14 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PM2.5        7782 non-null   float64\n",
            " 1   PM10         7782 non-null   float64\n",
            " 2   NO           7782 non-null   float64\n",
            " 3   NO2          7782 non-null   float64\n",
            " 4   NOx          7782 non-null   float64\n",
            " 5   NH3          7782 non-null   float64\n",
            " 6   CO           7782 non-null   float64\n",
            " 7   SO2          7782 non-null   float64\n",
            " 8   O3           7782 non-null   float64\n",
            " 9   Benzene      7782 non-null   float64\n",
            " 10  Toluene      7782 non-null   float64\n",
            " 11  Xylene       7782 non-null   float64\n",
            " 12  AQI          7782 non-null   float64\n",
            " 13  Air_Quality  7782 non-null   object \n",
            "dtypes: float64(13), object(1)\n",
            "memory usage: 851.3+ KB\n",
            "None\n",
            "Counter({'Very Poor': 1297, 'Poor': 1297, 'Moderate': 1297, 'Satisfactory': 1297, 'Severe': 1297, 'Good': 1297})\n",
            "The integer mapping:\n",
            " {'Good': 0, 'Moderate': 1, 'Poor': 2, 'Satisfactory': 3, 'Severe': 4, 'Very Poor': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 5 - Setting the optimizer\n",
        "\n",
        "Now that we’ve had a brief introduction to cross-entropy, we’ll see how to use it with our model.\n",
        "\n",
        "1. First, to specify the use of cross-entropy when optimizing the model, we **need to set the loss parameter to categorical_crossentropy of the Model**.compile() method.\n",
        "\n",
        "2. Second, we also need to **decide which metrics to use to evaluate our model**. For **classification**, we usually use **accuracy**. ***Accuracy calculates how often predictions equal labels and is expressed in percentages***. We will use this metric for our problem.\n",
        "\n",
        "Finally, we will use Adam as our optimizer because it’s effective here and is commonly used.\n",
        "\n",
        "To compile the model with all the specifications mentioned above we do the following:\n",
        "\n",
        "```python\n",
        "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "We are now ready to train our model."
      ],
      "metadata": {
        "id": "RY7G8cwmXqBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 5 - Setting the optimizer\n",
        "\n",
        "1. Compile your model instance model using the categorical_crossentropy loss, adam optimizer, and accuracy as the metrics."
      ],
      "metadata": {
        "id": "GvvzNQmhcFup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gVUw6BJIaVa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 6 - Train and evaluate the classification model\n",
        "\n",
        "To **train a model instance my_model on the training data my_data and training labels my_labels** we do the following:\n",
        "\n",
        "```python\n",
        "my_model.fit(my_data, my_labels, epochs=10, batch_size=1, verbose=1)\n",
        "```\n",
        "\n",
        "With the command above, we set the number of epochs to 10 and the batch size to 1. \n",
        "\n",
        "To see the progress of the training we set verbose to true (1).\n",
        "\n",
        "After the model is trained, we can evaluate it using the unseen test data my_test and test labels test_labels:\n",
        "\n",
        "```python\n",
        "loss, acc = my_model.evaluate(my_test, test_labels, verbose=0)\n",
        "```\n",
        "\n",
        "We take two outputs out of the .evaluate() function:\n",
        "\n",
        "* **the value of the loss** (categorical_crossentropy)\n",
        "* **accuracy** (as set in the metrics parameter of .compile())."
      ],
      "metadata": {
        "id": "DC3DRQpZa6xU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 6 - Train and evaluate the classification model\n",
        "\n",
        "1. Using the Model.fit() function, train your model instance model with the training data x_train and labels y_train, using 20 epochs, batch size of 4, and verbose set to 1.\n",
        "\n",
        "Note: Running this in the LE will take almost a full minute!"
      ],
      "metadata": {
        "id": "i60c-fhfcW8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=20, batch_size=4, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAfZC82ZbbR0",
        "outputId": "4adbc17d-c20b-4513-9f46-d7ce590bbb07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1946/1946 [==============================] - 4s 2ms/step - loss: 4.8628 - accuracy: 0.4352\n",
            "Epoch 2/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 1.3485 - accuracy: 0.5844\n",
            "Epoch 3/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 1.0347 - accuracy: 0.6295\n",
            "Epoch 4/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.9395 - accuracy: 0.6464\n",
            "Epoch 5/20\n",
            "1946/1946 [==============================] - 4s 2ms/step - loss: 0.8315 - accuracy: 0.6715\n",
            "Epoch 6/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.7945 - accuracy: 0.6838\n",
            "Epoch 7/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.7444 - accuracy: 0.6994\n",
            "Epoch 8/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.6963 - accuracy: 0.7128\n",
            "Epoch 9/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.6679 - accuracy: 0.7303\n",
            "Epoch 10/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.6560 - accuracy: 0.7388\n",
            "Epoch 11/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.6145 - accuracy: 0.7502\n",
            "Epoch 12/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.5784 - accuracy: 0.7661\n",
            "Epoch 13/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.5541 - accuracy: 0.7727\n",
            "Epoch 14/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.5274 - accuracy: 0.7819\n",
            "Epoch 15/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.5006 - accuracy: 0.7967\n",
            "Epoch 16/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.4708 - accuracy: 0.8075\n",
            "Epoch 17/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.4548 - accuracy: 0.8124\n",
            "Epoch 18/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.4338 - accuracy: 0.8214\n",
            "Epoch 19/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.4106 - accuracy: 0.8319\n",
            "Epoch 20/20\n",
            "1946/1946 [==============================] - 3s 2ms/step - loss: 0.3964 - accuracy: 0.8374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1378a44bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 7 - Additional evaluation statistics\n",
        "\n",
        "Sometimes having only accuracy reported is not enough or adequate. \n",
        "\n",
        "Accuracy is often used when data is balanced, meaning it contains an equal or almost equal number of samples from all the classes. \n",
        "\n",
        "However, **oftentimes data comes imbalanced**. For example in medicine, the rate of a disease is low. \n",
        "\n",
        "**In these cases**, ***reporting another metric such as F1-score is more adequate***.\n",
        "\n",
        "Frequently, especially in medicine, false negatives and false positives have different consequences. For example, in medicine, if we generate a false negative it means that we claim a patient doesn’t have a disease, while they actually have it — yikes! Luckily, an F1-score is a helpful way to evaluate our model based on how badly it makes false negative mistakes.\n",
        "\n",
        "To observe the F1-score of a trained model instance my_model, amongst other metrics, we use sklearn.metrics.classification_report:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "yhat_classes = np.argmax(my_model.predict(my_test), axis = -1)\n",
        "y_true = np.argmax(my_test_labels, axis=1)\n",
        "print(classification_report(y_true, yhat_classes))\n",
        "```\n",
        "\n",
        "In the code above we do the following:\n",
        "\n",
        "* ***predict classes for all test cases my_test*** using the **.predict()** method and assign the result to the yhat_classes variable.\n",
        "* using ***.argmax() convert the one-hot-encoded labels my_test_labels into the index of the class the sample belongs to***. The index corresponds to our class encoded as an integer.\n",
        "* use the ***.classification_report()*** method to **calculate all the metrics**."
      ],
      "metadata": {
        "id": "yedKcz_seV1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISES 7 - Additional evaluation statistics\n",
        "\n",
        "1. Using the Model.predict() method, get the predictions for your test data x_test using the trained model instance model. Assign the result to a variable called y_estimate.\n",
        "\n",
        "2. Using np.argmax() convert the one-hot encoded labels y_estimate into the index of the class each sample in the test data belongs to with the axis parameter set to 1. Assign the result to y_estimate.\n",
        "\n",
        "3. Using np.argmax() convert the one-hot encoded labels y_test into the index of the class each sample in the test data belongs to with the axis parameter set to 1. Assign the result to y_true.\n",
        "\n",
        "4. Using sklearn.metrics.classification_report, print additional metrics, such as F1-score calculated between the true y_true and estimated test data labels y_estimate.\n"
      ],
      "metadata": {
        "id": "1RWh1fYNfZUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_estimate = model.predict(x_test)\n",
        "\n",
        "#get additional statistics\n",
        "y_estimate = np.argmax(y_estimate, axis = 1)\n",
        "y_true = np.argmax(y_test, axis = 1)\n",
        "\n",
        "print(classification_report(y_true, y_estimate))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1V2XOKphLue",
        "outputId": "4f55131a-8039-41d1-d659-1cee26c76e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90       100\n",
            "           1       0.87      0.92      0.89       508\n",
            "           2       0.73      0.63      0.68       172\n",
            "           3       0.96      0.85      0.90       452\n",
            "           4       0.59      0.78      0.67        37\n",
            "           5       0.61      0.72      0.66       125\n",
            "\n",
            "    accuracy                           0.84      1394\n",
            "   macro avg       0.77      0.81      0.78      1394\n",
            "weighted avg       0.85      0.84      0.84      1394\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 8 - Classification loss alternative: sparse crossentropy\n",
        "\n",
        "As we saw before, categorical cross-entropy requires that we first integer-encode our categorical labels and then convert them to one-hot encodings using to_categorical(). \n",
        "\n",
        "**There is another type of loss** – **sparse categorical cross-entropy** – which is a computationally modified categorical cross-entropy loss **that allows you to leave the integer labels as they are and skip the entire procedure of encoding**.\n",
        "\n",
        "**Sparse categorical cross-entropy** is mathematically identical to categorical cross-entropy but **introduces some computational shortcuts** that **save time in memory as well as computation** because it **uses a single integer for a class**, rather than a whole vector. This is especially ***useful when we have data with many classes to predict***.\n",
        "\n",
        "We can specify the use of the sparse categorical crossentropy in the .compile() method:\n",
        "\n",
        "```python\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "Note the following changes: we make sure that our labels are just integer encoded using the LabelEncoder() but not converted into one-hot-encodings using .to_categorical(). \n",
        "\n",
        "Hence, we comment out the code that uses .to_categorical()."
      ],
      "metadata": {
        "id": "Zg9Ji9TQAPZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 8 - Classification loss alternative: sparse crossentropy\n",
        "\n",
        "Using the # symbol for comments, comment out the following lines of code (Line 36 and Line 37):\n",
        "\n",
        "```python\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, dtype = 'int64')\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, dtype = 'int64')\n",
        "```"
      ],
      "metadata": {
        "id": "0uVtCqaJAvNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import  InputLayer\n",
        "from tensorflow.keras.layers import  Dense\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "#your code here\n",
        "\n",
        "#train_data = pd.read_csv(\"air_quality_train.csv\")\n",
        "#test_data = pd.read_csv(\"air_quality_test.csv\")\n",
        "\n",
        "#print columns and their respective types\n",
        "print(train_data.info())\n",
        "#print the class distribution\n",
        "print(Counter(train_data[\"Air_Quality\"]))\n",
        "#extract the features from the training data\n",
        "x_train = train_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the training data\n",
        "y_train = train_data[\"Air_Quality\"]\n",
        "#extract the features from the test data\n",
        "x_test = test_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the test data\n",
        "y_test = test_data[\"Air_Quality\"]\n",
        "\n",
        "#encode the labels into integers\n",
        "le = LabelEncoder()\n",
        "#convert the integer encoded labels into binary vectors\n",
        "y_train=le.fit_transform(y_train.astype(str))\n",
        "y_test=le.transform(y_test.astype(str))\n",
        "#convert the integer encoded labels into binary vectors\n",
        "#we comment it here because we need only integer labels for\n",
        "#sparse cross-entropy\n",
        "#y_train = tensorflow.keras.utils.to_categorical(y_train, dtype = 'int64')\n",
        "#y_test = tensorflow.keras.utils.to_categorical(y_test, dtype = 'int64')\n",
        "\n",
        "#design the model\n",
        "model = Sequential()\n",
        "#add the input layer\n",
        "model.add(InputLayer(input_shape=(x_train.shape[1],)))\n",
        "#add a hidden layer\n",
        "model.add(Dense(10, activation='relu'))\n",
        "#add an output layer\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "#compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#train and evaluate the model\n",
        "model.fit(x_train, y_train, epochs = 20, batch_size = 16, verbose = 0)\n",
        "\n",
        "#get additional statistics\n",
        "y_estimate = model.predict(x_test, verbose=0)\n",
        "y_estimate = np.argmax(y_estimate, axis=1)\n",
        "print(classification_report(y_test, y_estimate))\n",
        "# Remember that with the loss entropy, the best is 0, so less is the best! \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38zev6NJA5Vf",
        "outputId": "496d0a7b-ce14-412c-c3a0-c452a04d307e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7782 entries, 0 to 7781\n",
            "Data columns (total 14 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PM2.5        7782 non-null   float64\n",
            " 1   PM10         7782 non-null   float64\n",
            " 2   NO           7782 non-null   float64\n",
            " 3   NO2          7782 non-null   float64\n",
            " 4   NOx          7782 non-null   float64\n",
            " 5   NH3          7782 non-null   float64\n",
            " 6   CO           7782 non-null   float64\n",
            " 7   SO2          7782 non-null   float64\n",
            " 8   O3           7782 non-null   float64\n",
            " 9   Benzene      7782 non-null   float64\n",
            " 10  Toluene      7782 non-null   float64\n",
            " 11  Xylene       7782 non-null   float64\n",
            " 12  AQI          7782 non-null   float64\n",
            " 13  Air_Quality  7782 non-null   object \n",
            "dtypes: float64(13), object(1)\n",
            "memory usage: 851.3+ KB\n",
            "None\n",
            "Counter({'Very Poor': 1297, 'Poor': 1297, 'Moderate': 1297, 'Satisfactory': 1297, 'Severe': 1297, 'Good': 1297})\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.95      0.73       100\n",
            "           1       0.86      0.70      0.77       508\n",
            "           2       0.55      0.73      0.63       172\n",
            "           3       0.82      0.76      0.79       452\n",
            "           4       0.54      0.70      0.61        37\n",
            "           5       0.64      0.65      0.64       125\n",
            "\n",
            "    accuracy                           0.74      1394\n",
            "   macro avg       0.67      0.75      0.70      1394\n",
            "weighted avg       0.76      0.74      0.74      1394\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY 10 - Tweak the model\n",
        "\n",
        "Now that we have run our code several times, we might be wondering if the model can be further improved.\n",
        "\n",
        "The first thing we can try is to increase the number of epochs. Having 20 epochs, as we previously had, is usually not enough. Try changing the number of epochs, for example, to 40 and see what happens. Increasing the number of epochs naturally makes the learning longer, but as you probably observed, the results are often much better.\n",
        "\n",
        "Other **hyperparameters you might consider changing are**: the batch size number of hidden layers number of units per hidden layer the learning rate of the optimizer the optimizer and so on."
      ],
      "metadata": {
        "id": "UNw86q-aDqHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 10 - Tweak the model\n",
        "\n",
        "1. Change the number of epochs from 20 to 30. Rerun the code and observe the results.\n",
        "\n",
        "Note: Running this in the LE will take some time!"
      ],
      "metadata": {
        "id": "m5KYKlTBD7_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import  InputLayer\n",
        "from tensorflow.keras.layers import  Dense\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "#train_data = pd.read_csv(\"air_quality_train.csv\")\n",
        "#test_data = pd.read_csv(\"air_quality_test.csv\")\n",
        "\n",
        "#print columns and their respective types\n",
        "print(train_data.info())\n",
        "#print the class distribution\n",
        "print(Counter(train_data[\"Air_Quality\"]))\n",
        "#extract the features from the training data\n",
        "x_train = train_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the training data\n",
        "y_train = train_data[\"Air_Quality\"]\n",
        "#extract the features from the test data\n",
        "x_test = test_data[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']]\n",
        "#extract the label column from the test data\n",
        "y_test = test_data[\"Air_Quality\"]\n",
        "\n",
        "#encode the labels into integers\n",
        "le = LabelEncoder()\n",
        "#convert the integer encoded labels into binary vectors\n",
        "y_train=le.fit_transform(y_train.astype(str))\n",
        "y_test=le.transform(y_test.astype(str))\n",
        "#convert the integer encoded labels into binary vectors\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, dtype = 'int64')\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, dtype = 'int64')\n",
        "\n",
        "#design the model\n",
        "model = Sequential()\n",
        "#add the input layer\n",
        "model.add(InputLayer(input_shape=(x_train.shape[1],)))\n",
        "#add a hidden layer\n",
        "model.add(Dense(10, activation='relu'))\n",
        "#add an output layer\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "#compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#train and evaluate the model\n",
        "model.fit(x_train, y_train, epochs = 30, batch_size = 16, verbose = 0)\n",
        "\n",
        "#get additional statistics\n",
        "y_estimate = model.predict(x_test)\n",
        "y_estimate = np.argmax(y_estimate, axis = 1)\n",
        "y_true = np.argmax(y_test, axis = 1)\n",
        "print(classification_report(y_true, y_estimate))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_k8-n8PD3SO",
        "outputId": "2fe6ccf5-ffb8-426c-8ee8-b59a31f737f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7782 entries, 0 to 7781\n",
            "Data columns (total 14 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PM2.5        7782 non-null   float64\n",
            " 1   PM10         7782 non-null   float64\n",
            " 2   NO           7782 non-null   float64\n",
            " 3   NO2          7782 non-null   float64\n",
            " 4   NOx          7782 non-null   float64\n",
            " 5   NH3          7782 non-null   float64\n",
            " 6   CO           7782 non-null   float64\n",
            " 7   SO2          7782 non-null   float64\n",
            " 8   O3           7782 non-null   float64\n",
            " 9   Benzene      7782 non-null   float64\n",
            " 10  Toluene      7782 non-null   float64\n",
            " 11  Xylene       7782 non-null   float64\n",
            " 12  AQI          7782 non-null   float64\n",
            " 13  Air_Quality  7782 non-null   object \n",
            "dtypes: float64(13), object(1)\n",
            "memory usage: 851.3+ KB\n",
            "None\n",
            "Counter({'Very Poor': 1297, 'Poor': 1297, 'Moderate': 1297, 'Satisfactory': 1297, 'Severe': 1297, 'Good': 1297})\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88       100\n",
            "           1       0.82      0.79      0.81       508\n",
            "           2       0.50      0.64      0.56       172\n",
            "           3       0.96      0.78      0.86       452\n",
            "           4       0.58      0.84      0.69        37\n",
            "           5       0.59      0.73      0.65       125\n",
            "\n",
            "    accuracy                           0.78      1394\n",
            "   macro avg       0.72      0.78      0.74      1394\n",
            "weighted avg       0.80      0.78      0.78      1394\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Congrats! You just created your first classification model using tabular data. Moreover, you performed multi-class classification! In this lesson, you learned:\n",
        "\n",
        "* The task of classification and what the main difference is between the binary and multi-class classification\n",
        "\n",
        "* How to calculate cross-entropy in practice as well as how to interpret it and use it for classification.\n",
        "\n",
        "* How to analyze your data using pandas functionalities, and see the distribution of the categories using collections.Counter(), which might be useful for seeing if the data is imbalanced.\n",
        "\n",
        "* How to prepare the data for classification by encoding the labels using sklearn.preprocessing.LabelEncoder() and converting them to one-hot encoding format necessary for the loss function using tensorflow.keras.utils.to_categorical().\n",
        "\n",
        "* How to design a TensorFlow with Keras deep learning model to perform classification focusing on the final (output) layer that needs to have a softmax activation function.\n",
        "\n",
        "* How to initialize the optimizer by using the categorical_cross_entropy loss and accuracy as the learning metrics.\n",
        "\n",
        "* How to train and evaluate your model.\n",
        "\n",
        "* How to use an alternative loss function sparse_categorical_crossentropy that allows you to keep your labels integer encoded and skip converting them into one-hot encoding.\n",
        "\n",
        "* How to tweak the model to see if the performance can be improved."
      ],
      "metadata": {
        "id": "8uzQNDkzEN2X"
      }
    }
  ]
}