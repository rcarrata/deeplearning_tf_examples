{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_Classification_Project_Heart_Failure.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO++jM8ek9bdgAv41xK9R5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcarrata/deeplearning_tf_examples/blob/master/8_Classification_Project_Heart_Failure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Project\n",
        "\n",
        "In this project, you will use a dataset from Kaggle to predict the survival of patients with heart failure from serum creatinine and ejection fraction, and other factors such as age, anemia, diabetes, and so on.\n",
        "\n",
        "Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Heart failure is a common event caused by CVDs, and this dataset contains 12 features that can be used to predict mortality by heart failure.\n",
        "\n",
        "Most cardiovascular diseases can be prevented by addressing behavioral risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity, and harmful alcohol use using population-wide strategies.\n",
        "\n",
        "People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidemia, or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
        "\n",
        "[Kaggle Source Data](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data)"
      ],
      "metadata": {
        "id": "62kKGIRQbj2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A - Loading the data\n",
        "\n",
        "1. Using pandas.read_csv(), load the data from heart_failure.csv to a pandas DataFrame object. Assign the resulting DataFrame to a variable called data.\n",
        "\n",
        "2. Use the DataFrame.info() method to print all the columns and their types of the DataFrame instance data.\n",
        "\n",
        "3. Print the distribution of the death_event column in the data DataFrame class using collections.Counter. This is the column you will need to predict.\n",
        "\n",
        "4. Extract the label column death_event from the data DataFrame and assign the result to a variable called y.\n",
        "\n",
        "5. Extract the features columns ['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time'] from the DataFrame instance data and assign the result to a variable called x."
      ],
      "metadata": {
        "id": "8tECYsVq6fKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/Colab/Classification/heart_failure.csv\"\n",
        "\n",
        "root_folder = \"/content/drive/My Drive/Colab/\"\n",
        "project_folder = \"Classification/\"\n",
        "csv_file = \"heart_failure.csv\"\n",
        "\n",
        "csv_data = root_folder + project_folder + csv_file\n",
        "print(csv_data)\n",
        "\n",
        "data = pd.read_csv(csv_data)\n",
        "\n",
        "from google.colab.data_table import DataTable\n",
        "DataTable.max_columns = 60\n",
        "\n",
        "#print the class distribution\n",
        "print(\"## Dataframe.Info\")\n",
        "print(data.info())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"## Death_Events distribution\")\n",
        "print('DEATH_EVENT',Counter(data[\"DEATH_EVENT\"]))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Extract the label column\n",
        "y = data[\"DEATH_EVENT\"]\n",
        "\n",
        "# Extract the feature columns\n",
        "x = data[['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time']]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssmCdAuhYnBI",
        "outputId": "6bbbc4f8-b392-413d-fb4c-166abb406b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'/content/drive/My Drive/Colab/Classification/heart_failure.csv'\n",
            "/content/drive/My Drive/Colab/Classification/heart_failure.csv\n",
            "## Dataframe.Info\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 299 entries, 0 to 298\n",
            "Data columns (total 13 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   age                       299 non-null    float64\n",
            " 1   anaemia                   299 non-null    int64  \n",
            " 2   creatinine_phosphokinase  299 non-null    int64  \n",
            " 3   diabetes                  299 non-null    int64  \n",
            " 4   ejection_fraction         299 non-null    int64  \n",
            " 5   high_blood_pressure       299 non-null    int64  \n",
            " 6   platelets                 299 non-null    float64\n",
            " 7   serum_creatinine          299 non-null    float64\n",
            " 8   serum_sodium              299 non-null    int64  \n",
            " 9   sex                       299 non-null    int64  \n",
            " 10  smoking                   299 non-null    int64  \n",
            " 11  time                      299 non-null    int64  \n",
            " 12  DEATH_EVENT               299 non-null    int64  \n",
            "dtypes: float64(3), int64(10)\n",
            "memory usage: 30.5 KB\n",
            "None\n",
            "\n",
            "\n",
            "## Death_Events distribution\n",
            "DEATH_EVENT Counter({0: 203, 1: 96})\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B - Data preprocessing\n",
        "\n",
        "6. Use the pandas.get_dummies() function to convert the categorical features in the DataFrame instance x to one-hot encoding vectors and assign the result back to variable x. **NOTE**: Not using the sklearn.preprocessing.LabelEncoder because we have the label vectors in the column of Labels, all transformed in integers from 0 to 1\n",
        "\n",
        "7. Use the sklearn.model_selection.train_test_split() method to split the data into training features, test features, training labels, and test labels, respectively. To the test_size parameter assign the percentage of data you wish to put in the test data, and use any value for the random_state parameter. Store the results of the function to X_train, X_test, Y_train, Y_test variables, making sure you use this order.\n",
        "\n",
        "8. Initialize a ColumnTransformer object by using StandardScaler to scale the numeric features in the dataset: ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']. Assign the resulting object to a variable called ct.\n",
        "\n",
        "9. Use the ColumnTransformer.fit_transform() function to train the scaler instance ct on the training data X_train and assign the result back to X_train.\n",
        "\n",
        "10. Use the ColumnTransformer.transform() to scale the test data instance X_test using the trained scaler ct, and assign the result back to X_test. Note - https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn\n",
        "\n",
        "  ```python\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  sc = StandardScaler()\n",
        "  sc.fit_transform(X_train)\n",
        "  sc.transform(X_test)\n",
        "  ```"
      ],
      "metadata": {
        "id": "Oy0blAIobjPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Use pandas.get_dummies(DataFrame) to apply one-hot-encoding on all the categorical columns. Assign the result of the encoding back to the features variable.\n",
        "x = pd.get_dummies(x)\n",
        "print(x.head())\n",
        "\n",
        "# Not using the sklearn.preprocessing.LabelEncoder because we have the label vectors in the column of Labels, all transformed in integers from 0 to 1\n",
        "\n",
        "# sklearn.model_selection.train_test_split() method\n",
        "# test_size - If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split.\n",
        "# random_state - Controls the shuffling applied to the data before applying the split.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "# When initializing ColumnTransformer make sure to list all of the numerical features you have in your dataset. Or use DataFrame.select_dtypes() to select float64 or int64 feature types automatically.\n",
        "numerical_features = x.select_dtypes(include=['float64', 'int64'])\n",
        "numerical_columns = numerical_features.columns\n",
        "print(\"## Selecting Numerical Columns only to use ColumnTransformer on them\")\n",
        "print(numerical_columns)\n",
        "print(\"\\n\")\n",
        "\n",
        "ct = ColumnTransformer([('numeric', StandardScaler(), numerical_columns)], remainder='passthrough')\n",
        "\n",
        "X_train = ct.fit_transform(X_train)\n",
        "#print(X_train)\n",
        "\n",
        "X_test = ct.transform(X_test)\n",
        "#print(X_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSMyEkYXkdtV",
        "outputId": "7171bef6-365c-47f1-b5be-4fbe81b79bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
            "0  75.0        0                       582         0                 20   \n",
            "1  55.0        0                      7861         0                 38   \n",
            "2  65.0        0                       146         0                 20   \n",
            "3  50.0        1                       111         0                 20   \n",
            "4  65.0        1                       160         1                 20   \n",
            "\n",
            "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
            "0                    1  265000.00               1.9           130    1   \n",
            "1                    0  263358.03               1.1           136    1   \n",
            "2                    0  162000.00               1.3           129    1   \n",
            "3                    0  210000.00               1.9           137    1   \n",
            "4                    0  327000.00               2.7           116    0   \n",
            "\n",
            "   smoking  time  \n",
            "0        0     4  \n",
            "1        0     6  \n",
            "2        1     7  \n",
            "3        0     7  \n",
            "4        0     8  \n",
            "## Selecting Numerical Columns only to use ColumnTransformer on them\n",
            "Index(['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n",
            "       'ejection_fraction', 'high_blood_pressure', 'platelets',\n",
            "       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time'],\n",
            "      dtype='object')\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C - Prepare labels for classification\n",
        "\n",
        "11. Initialize an instance of LabelEncoder and assign it to a variable called le. NOTE - Use the LabelEncoder.fit_transform() method to encode the label vector y_train into integers and assign the result back to the y_train variable.\n",
        "\n",
        "12. Using the LabelEncoder.fit_transform() function, fit the encoder instance le to the training labels Y_train, while at the same time converting the training labels according to the trained encoder.\n",
        "\n",
        "13. Using the LabelEncoder.transform() function, encode the test labels Y_test using the trained encoder le.\n",
        "\n",
        "14. Using the tensorflow.keras.utils.to_categorical() function, transform the encoded training labels Y_train into a binary vector and assign the result back to Y_train."
      ],
      "metadata": {
        "id": "5Fdi5qJOv5Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "Y_train = le.fit_transform(Y_train.astype(str))\n",
        "Y_test = le.transform(Y_test.astype(str))\n",
        "\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "#print(Y_train)\n",
        "#print(Y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE4Tdgs9wg3_",
        "outputId": "6f2041ce-3633-4aa1-ae8a-1c969af19b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D - Design the model\n",
        "\n",
        "16. Initialize a tensorflow.keras.models.Sequential model instance called model.\n",
        "\n",
        "17. Create an input layer instance of tensorflow.keras.layers.InputLayer and add it to the model instance model using the Model.add() function.\n",
        "\n",
        "18. Create a hidden layer instance of tensorflow.keras.layers.Dense with relu activation function and 12 hidden neurons, and add it to the model instance model.\n",
        "\n",
        "19. Create an output layer instance of tensorflow.keras.layers.Dense with a softmax activation function (because of classification) with the number of neurons corresponding to the number of classes in the dataset.\n",
        "\n",
        "20. Using the Model.compile() function, compile the model instance model using the categorical_crossentropy loss, adam optimizer and accuracy as metrics."
      ],
      "metadata": {
        "id": "53RNhbml1AJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import  InputLayer\n",
        "from tensorflow.keras.layers import  Dense\n",
        "\n",
        "#design the model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer Instance\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1],)))\n",
        "\n",
        "# Input Layer\n",
        "model.add(Dense(10, activation='relu'))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Dense(12, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "# The number of neurons corresponding to the number of classes in a dataset is 2, DEATH or NOT_DEATH\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# setting the optimizer (in other exercise worked with categorical_crossentropy)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lLISAmHt1KNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E - Train and Evaluate the model\n",
        "\n",
        "21. Using the Model.fit() function, fit the model instance model to the training data X_train and training labels Y_train. Set the number of epochs to 100 and the batch size parameter to 16.\n",
        "\n",
        "22. Using the Model.evaluate() function, evaluate the trained model instance model on the test data X_test and test labels Y_test. Assign the result to a variable called loss (representing the final loss value) and a variable called acc (representing the accuracy metrics), respectively.\n",
        "\n",
        "23. How to choose the Loss Functions - https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
        "Loss better when it's close to 0. The score is minimized and a perfect cross-entropy value is 0."
      ],
      "metadata": {
        "id": "mm7qkMpj3-T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import  InputLayer\n",
        "from tensorflow.keras.layers import  Dense\n",
        "\n",
        "\n",
        "# Fit the model with training data\n",
        "# Workaround\n",
        "print(\"\\n\")\n",
        "print(\"## Fitting the model with Training Data\")\n",
        "model.fit(X_train, Y_train, epochs=100, batch_size=16)\n",
        "\n",
        "# Evaluate the model with unseen Test Data\n",
        "print(\"\\n\")\n",
        "print(\"## Evaluate the model with Test Data\")\n",
        "loss, acc= model.evaluate(X_test, Y_test, verbose = 0)\n",
        "print(\"Loss\", loss, \"Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNhDjkjD5jkj",
        "outputId": "65331873-2bef-4992-a0e0-9a1adac06a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "## Fitting the model with Training Data\n",
            "Epoch 1/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7666 - accuracy: 0.4928\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7313 - accuracy: 0.5550\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.7045 - accuracy: 0.6220\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.6828 - accuracy: 0.6507\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6634 - accuracy: 0.6794\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6473 - accuracy: 0.7033\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6331 - accuracy: 0.7081\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6202 - accuracy: 0.7177\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6073 - accuracy: 0.7225\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5946 - accuracy: 0.7368\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5825 - accuracy: 0.7225\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5695 - accuracy: 0.7273\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7273\n",
            "Epoch 14/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.7368\n",
            "Epoch 15/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7464\n",
            "Epoch 16/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7560\n",
            "Epoch 17/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7703\n",
            "Epoch 18/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7751\n",
            "Epoch 19/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.7656\n",
            "Epoch 20/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7656\n",
            "Epoch 21/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7703\n",
            "Epoch 22/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7799\n",
            "Epoch 23/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.7799\n",
            "Epoch 24/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.7943\n",
            "Epoch 25/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.8086\n",
            "Epoch 26/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8182\n",
            "Epoch 27/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8278\n",
            "Epoch 28/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8325\n",
            "Epoch 29/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8373\n",
            "Epoch 30/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8373\n",
            "Epoch 31/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8325\n",
            "Epoch 32/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8373\n",
            "Epoch 33/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8469\n",
            "Epoch 34/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.8469\n",
            "Epoch 35/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8565\n",
            "Epoch 36/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3645 - accuracy: 0.8612\n",
            "Epoch 37/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8612\n",
            "Epoch 38/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3545 - accuracy: 0.8708\n",
            "Epoch 39/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8612\n",
            "Epoch 40/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8517\n",
            "Epoch 41/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.8660\n",
            "Epoch 42/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8708\n",
            "Epoch 43/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8708\n",
            "Epoch 44/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8756\n",
            "Epoch 45/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8660\n",
            "Epoch 46/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8612\n",
            "Epoch 47/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8612\n",
            "Epoch 48/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8612\n",
            "Epoch 49/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8612\n",
            "Epoch 50/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8756\n",
            "Epoch 51/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8708\n",
            "Epoch 52/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.8756\n",
            "Epoch 53/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.8756\n",
            "Epoch 54/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3122 - accuracy: 0.8756\n",
            "Epoch 55/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8756\n",
            "Epoch 56/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8756\n",
            "Epoch 57/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8756\n",
            "Epoch 58/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8804\n",
            "Epoch 59/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3044 - accuracy: 0.8804\n",
            "Epoch 60/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.8708\n",
            "Epoch 61/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8756\n",
            "Epoch 62/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8756\n",
            "Epoch 63/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.8756\n",
            "Epoch 64/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2976 - accuracy: 0.8756\n",
            "Epoch 65/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2955 - accuracy: 0.8804\n",
            "Epoch 66/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.8756\n",
            "Epoch 67/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.8804\n",
            "Epoch 68/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8756\n",
            "Epoch 69/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2908 - accuracy: 0.8852\n",
            "Epoch 70/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8852\n",
            "Epoch 71/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.8852\n",
            "Epoch 72/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8852\n",
            "Epoch 73/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.8756\n",
            "Epoch 74/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8804\n",
            "Epoch 75/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.8804\n",
            "Epoch 76/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2796 - accuracy: 0.8804\n",
            "Epoch 77/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8852\n",
            "Epoch 78/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8852\n",
            "Epoch 79/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8852\n",
            "Epoch 80/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2742 - accuracy: 0.8852\n",
            "Epoch 81/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8852\n",
            "Epoch 82/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2708 - accuracy: 0.8756\n",
            "Epoch 83/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.8804\n",
            "Epoch 84/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8804\n",
            "Epoch 85/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.8852\n",
            "Epoch 86/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2709 - accuracy: 0.8852\n",
            "Epoch 87/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8852\n",
            "Epoch 88/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8852\n",
            "Epoch 89/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8900\n",
            "Epoch 90/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.8947\n",
            "Epoch 91/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.8947\n",
            "Epoch 92/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8900\n",
            "Epoch 93/100\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8900\n",
            "Epoch 94/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2595 - accuracy: 0.8852\n",
            "Epoch 95/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.8852\n",
            "Epoch 96/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8852\n",
            "Epoch 97/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.8852\n",
            "Epoch 98/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.8852\n",
            "Epoch 99/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.8900\n",
            "Epoch 100/100\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.8900\n",
            "\n",
            "\n",
            "## Evaluate the model with Test Data\n",
            "Loss 0.44586825370788574 Accuracy: 0.8333333134651184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F - Generating a classification report\n",
        "\n",
        "23. Use the Model.predict() to get the predictions for the test data X_test with the trained model instance model. Assign the result to a variable called y_estimate.\n",
        "\n",
        "24. Use the numpy.argmax() method to select the indices of the true classes for each label encoding in y_estimate. Assign the result to a variable called y_estimate.\n",
        "\n",
        "25. Use the numpy.argmax() method to select the indices of the true classes for each label encoding in Y_test. Assign the result to a variable called y_true.\n",
        "\n",
        "# "
      ],
      "metadata": {
        "id": "GPCwHx6V7VSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get the predictions for the test data X_test\n",
        "print(\"\\n\")\n",
        "print(\"## Predictions for the Test Data (X_test)\")\n",
        "y_estimate = model.predict(X_test)\n",
        "print(y_estimate)\n",
        "\n",
        "# select the indices of the true classes for each label encoding in y_estimate\n",
        "y_estimate = np.argmax(y_estimate, axis = 1)\n",
        "\n",
        "# select the indices of the true classes for each label encoding in Y_test\n",
        "y_true = np.argmax(Y_test, axis = 1)\n",
        "\n",
        "# Print additional metrics, such as F1-score\n",
        "print(\"\\n\")\n",
        "print(\"## Print additional Metrics\")\n",
        "print(classification_report(y_true, y_estimate))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9sU5LzG8Msf",
        "outputId": "09ec6bb8-719d-4ac4-e41d-a66a39baa119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "## Predictions for the Test Data (X_test)\n",
            "[[9.56739724e-01 4.32602279e-02]\n",
            " [9.72063184e-01 2.79368497e-02]\n",
            " [6.52504981e-01 3.47495049e-01]\n",
            " [9.89320755e-01 1.06792599e-02]\n",
            " [7.70613134e-01 2.29386896e-01]\n",
            " [8.22895169e-01 1.77104831e-01]\n",
            " [5.08633375e-01 4.91366655e-01]\n",
            " [9.69651103e-01 3.03489100e-02]\n",
            " [4.51679498e-01 5.48320472e-01]\n",
            " [6.43049300e-01 3.56950670e-01]\n",
            " [9.73917782e-01 2.60822382e-02]\n",
            " [5.13367541e-02 9.48663235e-01]\n",
            " [9.02368009e-01 9.76319760e-02]\n",
            " [9.67137158e-01 3.28628160e-02]\n",
            " [1.57343045e-01 8.42656970e-01]\n",
            " [5.78066260e-02 9.42193389e-01]\n",
            " [9.93327379e-01 6.67258026e-03]\n",
            " [8.11383247e-01 1.88616812e-01]\n",
            " [9.97530282e-01 2.46973406e-03]\n",
            " [9.96634424e-01 3.36561142e-03]\n",
            " [4.93412077e-01 5.06587863e-01]\n",
            " [9.65832546e-02 9.03416693e-01]\n",
            " [1.84259519e-01 8.15740407e-01]\n",
            " [6.90233529e-01 3.09766471e-01]\n",
            " [5.61238289e-01 4.38761681e-01]\n",
            " [9.82451975e-01 1.75480805e-02]\n",
            " [7.90069282e-01 2.09930673e-01]\n",
            " [9.95841086e-01 4.15890338e-03]\n",
            " [9.97981012e-01 2.01901793e-03]\n",
            " [8.23908806e-01 1.76091194e-01]\n",
            " [8.19271982e-01 1.80728018e-01]\n",
            " [2.84398228e-01 7.15601742e-01]\n",
            " [4.69717830e-01 5.30282199e-01]\n",
            " [6.25802755e-01 3.74197215e-01]\n",
            " [4.22455192e-01 5.77544808e-01]\n",
            " [5.22182524e-01 4.77817476e-01]\n",
            " [9.37476575e-01 6.25234097e-02]\n",
            " [6.22370839e-01 3.77629161e-01]\n",
            " [9.99106586e-01 8.93381715e-04]\n",
            " [4.33776062e-03 9.95662272e-01]\n",
            " [9.86105680e-01 1.38943410e-02]\n",
            " [7.94764578e-01 2.05235422e-01]\n",
            " [8.17984819e-01 1.82015121e-01]\n",
            " [8.91172647e-01 1.08827315e-01]\n",
            " [3.15393880e-03 9.96846020e-01]\n",
            " [9.50613081e-01 4.93869297e-02]\n",
            " [9.99290109e-01 7.09853019e-04]\n",
            " [9.57651615e-01 4.23483513e-02]\n",
            " [9.60441768e-01 3.95582393e-02]\n",
            " [3.48655164e-01 6.51344895e-01]\n",
            " [7.67197728e-01 2.32802272e-01]\n",
            " [9.48999584e-01 5.10004349e-02]\n",
            " [8.15063775e-01 1.84936225e-01]\n",
            " [8.99373353e-01 1.00626692e-01]\n",
            " [4.16019671e-02 9.58398044e-01]\n",
            " [4.84517157e-01 5.15482843e-01]\n",
            " [7.77318776e-01 2.22681284e-01]\n",
            " [5.84557690e-02 9.41544175e-01]\n",
            " [9.96417403e-01 3.58264591e-03]\n",
            " [7.59470761e-01 2.40529224e-01]\n",
            " [9.10580009e-02 9.08941925e-01]\n",
            " [9.67493057e-01 3.25069688e-02]\n",
            " [9.16065693e-01 8.39342996e-02]\n",
            " [9.98252571e-01 1.74745638e-03]\n",
            " [9.98591244e-01 1.40871143e-03]\n",
            " [9.98247504e-01 1.75255956e-03]\n",
            " [9.74092185e-01 2.59078033e-02]\n",
            " [7.70812869e-01 2.29187116e-01]\n",
            " [8.86388198e-02 9.11361098e-01]\n",
            " [1.94732964e-01 8.05267096e-01]\n",
            " [8.08382154e-01 1.91617861e-01]\n",
            " [9.19904053e-01 8.00959319e-02]\n",
            " [9.27527308e-01 7.24726617e-02]\n",
            " [1.36104703e-01 8.63895297e-01]\n",
            " [1.30821943e-01 8.69178057e-01]\n",
            " [9.74854708e-01 2.51452848e-02]\n",
            " [2.98548311e-01 7.01451659e-01]\n",
            " [5.98570287e-01 4.01429743e-01]\n",
            " [8.05964530e-01 1.94035456e-01]\n",
            " [3.55216056e-01 6.44783914e-01]\n",
            " [9.71616566e-01 2.83834245e-02]\n",
            " [9.85982955e-01 1.40170511e-02]\n",
            " [9.99932408e-01 6.75655028e-05]\n",
            " [9.82279241e-01 1.77207291e-02]\n",
            " [9.96311009e-01 3.68902343e-03]\n",
            " [5.03501415e-01 4.96498674e-01]\n",
            " [6.41729593e-01 3.58270496e-01]\n",
            " [9.14353490e-01 8.56464654e-02]\n",
            " [9.48066056e-01 5.19339815e-02]\n",
            " [9.99426246e-01 5.73839236e-04]]\n",
            "\n",
            "\n",
            "## Print additional Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88        62\n",
            "           1       0.78      0.64      0.71        28\n",
            "\n",
            "    accuracy                           0.83        90\n",
            "   macro avg       0.82      0.78      0.79        90\n",
            "weighted avg       0.83      0.83      0.83        90\n",
            "\n"
          ]
        }
      ]
    }
  ]
}