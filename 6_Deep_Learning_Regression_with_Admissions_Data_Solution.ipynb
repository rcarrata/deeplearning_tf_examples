{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Deep_Learning_Regression_with_Admissions_Data_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBp0rZFokbV7dJEMWSlPvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcarrata/deeplearning_tf_examples/blob/master/6_Deep_Learning_Regression_with_Admissions_Data_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Regression with Admissions Data\n",
        "\n",
        "For this project, you will create a deep learning regression model that predicts the likelihood that a student applying to graduate school will be accepted based on various application factors (such as test scores).\n",
        "\n",
        "By analyzing the parameters in this graduate admissions dataset, you will use TensorFlow with Keras to create a regression model that can evaluate the chances of an applicant being admitted. You hope this will give you further insight into the graduate admissions world and improve your test prep strategy."
      ],
      "metadata": {
        "id": "kNLuECMmxt4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise Step by Step \n",
        "\n",
        "1. If you take a look at admissions_data.csv, you’ll see parameters that admissions officers commonly use to evaluate university applicants. This data is from Kaggle and provides information about 500 applications for various universities and what their chance of admittance is.\n",
        "\n",
        " This is a regression problem because the probability of being admitted is a continuous label between 0 and 1.\n",
        "\n",
        " Load the csv file into a DataFrame and investigate the rows and columns to get familiarity with the dataset.\n",
        "\n",
        " To get more information about each parameter in admissions_data.csv click the hint below.\n",
        "\n",
        "2. Split it up the data into feature parameters and the labels.\n",
        "\n",
        "You are creating a model that predicts an applicant’s likelihood of being admitted to a master’s program, so take some time to look at the features of your model and which column you are trying to predict. Also consider if there are any dataset features that should not be included as a predictor.\n",
        "\n",
        "Make sure all of your variables are numerical.\n",
        "\n",
        "If there are any categorical variables, be sure to map them to numerical values, using techniques such as one-hot-encoding, so they can be used in a regression analysis.\n",
        "\n",
        "3. Since you are creating a learning model, you must have a training set and a test set. Remember that this allows you to measure the effectiveness of your model.\n",
        "\n",
        " You have created two DataFrames: one for features DataFrame and one for labels. Now, you must split each of these into a training set and a test set.\n",
        "\n",
        " If you need a refresher on splitting a train and test set, use scikit-learn’s user guide and any other online resources for help!\n",
        "\n",
        " Click the hint below if you need any other guidance.\n",
        "\n",
        "4. If you look through the admissions_data.csv, you may notice that there are many different scales being used. For example, the GRE Score is out of 340 while the University Rating is out of 5. Can you imagine why this might be a problem when using a regression learning model?\n",
        "\n",
        " You should either scale or normalize your data so that all columns/features have equal weight in the learning model.\n",
        "\n",
        "5. Create a neural network model to perform a regression analysis on the admission data.\n",
        "\n",
        " When designing your own neural network model, consider the following:\n",
        "\n",
        " * The shape of your input\n",
        " * Adding hidden layers as well as how many neurons they have\n",
        " * Including activation functions\n",
        " * The type of loss function and metrics you use\n",
        " * The type of gradient descent optimizer you use\n",
        " * Your learning rate\n",
        "\n",
        "6. It’s time to test out the model you created!\n",
        "\n",
        " Fit your model with your training set and test it out with your test set.\n",
        "\n",
        " It’s okay if it is not that accurate right now. You can play around with your model and tweak it to increase its accuracy.\n",
        "\n",
        "7. You have tested out your model. Now is the time to adjust your model’s hyperparameters. You have a lot of choices to make. You can choose:\n",
        "\n",
        " * the number of epochs\n",
        " * the size of your batch_size\n",
        " * to add more hidden layers\n",
        " * your type of optimizer and/or activation functions.\n",
        "\n",
        " Have fun in the hyperparameter playground. Test things out and see what works and what does not work. See what makes your model optimized between speed and accuracy. You have complete creative power!\n",
        "\n",
        "\n",
        "9. Using the Matplotlib Library , see if you can plot the model loss per epoch as well as the mean-average error per epoch for both training and validation data. This will give you an insight into how the model performs better over time and can also help you figure out better ways to tune your hyperparameters.\n",
        "\n",
        " Because of the way Matplotlib plots are displayed in the learning environment, please use fig.savefig('static/images/my_plots.png') at the end of your graphing code to render the plot in the browser. If you wish to display multiple plots, you can use .subplot() or .add_subplot() methods in the Matplotlib library to depict multiple plots in one figure.\n",
        "\n",
        " Use the hint below if you have any struggles with displaying these graphs.\n",
        "\n",
        "10. Let’s say you wanted to evaluate how strongly the features in admissions.csv predict an applicant’s admission into a graduate program. We can use something called an R-squared value. It is also known as the coefficient of determination; feel free to explore more about it here.\n",
        "\n",
        " Basically, we can use this calculation to see how well the features in our regression model make predictions. An R-squared value near close to 1 suggests a well-fit regression model, while a value closer to 0 suggests that the regression model does not fit the data well.\n",
        "\n",
        " See if you can apply this to your model after it has been evaluated using a .predict() method on your features_test_set and the r2_score() function on your labels_test_set. Both of these functions are from the scikit-learn library."
      ],
      "metadata": {
        "id": "LU5CQ5yO3xcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "bR4f681Hxpug",
        "outputId": "02859b47-f644-4582-c4da-fbf020196a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'/content/drive/My Drive/Colab/Admissions/admissions_data.csv'\n",
            "/content/drive/My Drive/Colab/Admissions/admissions_data.csv\n",
            "## Dataset Head\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
              "0           1        337          118                  4  4.5   4.5  9.65   \n",
              "1           2        324          107                  4  4.0   4.5  8.87   \n",
              "2           3        316          104                  3  3.0   3.5  8.00   \n",
              "3           4        322          110                  3  3.5   2.5  8.67   \n",
              "4           5        314          103                  2  2.0   3.0  8.21   \n",
              "\n",
              "   Research  Chance of Admit   \n",
              "0         1              0.92  \n",
              "1         1              0.76  \n",
              "2         1              0.72  \n",
              "3         1              0.80  \n",
              "4         0              0.65  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7b4db5e-3b9b-44c1-b040-57820b9d4338\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE Score</th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>University Rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Research</th>\n",
              "      <th>Chance of Admit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>337</td>\n",
              "      <td>118</td>\n",
              "      <td>4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>9.65</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>324</td>\n",
              "      <td>107</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8.87</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>316</td>\n",
              "      <td>104</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>322</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>8.67</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>314</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7b4db5e-3b9b-44c1-b040-57820b9d4338')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7b4db5e-3b9b-44c1-b040-57820b9d4338 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7b4db5e-3b9b-44c1-b040-57820b9d4338');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/Colab/Admissions/admissions_data.csv\"\n",
        "\n",
        "root_folder = \"/content/drive/My Drive/Colab/\"\n",
        "project_folder = \"Admissions/\"\n",
        "csv_file = \"admissions_data.csv\"\n",
        "\n",
        "csv_data = root_folder + project_folder + csv_file\n",
        "print(csv_data)\n",
        "\n",
        "admissions_data = pd.read_csv(csv_data)\n",
        "\n",
        "from google.colab.data_table import DataTable\n",
        "DataTable.max_columns = 60\n",
        "\n",
        "print(\"## Dataset Head\")\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"## Dataset Describe\")\n",
        "dataset.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "t3EtoVs6Rcwi",
        "outputId": "8477fcf1-9f2a-40af-8878-e03a41d26803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Dataset Describe\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Serial No.   GRE Score  TOEFL Score  University Rating         SOP  \\\n",
              "count  500.000000  500.000000   500.000000         500.000000  500.000000   \n",
              "mean   250.500000  316.472000   107.192000           3.114000    3.374000   \n",
              "std    144.481833   11.295148     6.081868           1.143512    0.991004   \n",
              "min      1.000000  290.000000    92.000000           1.000000    1.000000   \n",
              "25%    125.750000  308.000000   103.000000           2.000000    2.500000   \n",
              "50%    250.500000  317.000000   107.000000           3.000000    3.500000   \n",
              "75%    375.250000  325.000000   112.000000           4.000000    4.000000   \n",
              "max    500.000000  340.000000   120.000000           5.000000    5.000000   \n",
              "\n",
              "            LOR         CGPA    Research  Chance of Admit   \n",
              "count  500.00000  500.000000  500.000000         500.00000  \n",
              "mean     3.48400    8.576440    0.560000           0.72174  \n",
              "std      0.92545    0.604813    0.496884           0.14114  \n",
              "min      1.00000    6.800000    0.000000           0.34000  \n",
              "25%      3.00000    8.127500    0.000000           0.63000  \n",
              "50%      3.50000    8.560000    1.000000           0.72000  \n",
              "75%      4.00000    9.040000    1.000000           0.82000  \n",
              "max      5.00000    9.920000    1.000000           0.97000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4253f94-f1d4-4a1d-9b66-3a1250b9716a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE Score</th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>University Rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Research</th>\n",
              "      <th>Chance of Admit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.00000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>250.500000</td>\n",
              "      <td>316.472000</td>\n",
              "      <td>107.192000</td>\n",
              "      <td>3.114000</td>\n",
              "      <td>3.374000</td>\n",
              "      <td>3.48400</td>\n",
              "      <td>8.576440</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.72174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>144.481833</td>\n",
              "      <td>11.295148</td>\n",
              "      <td>6.081868</td>\n",
              "      <td>1.143512</td>\n",
              "      <td>0.991004</td>\n",
              "      <td>0.92545</td>\n",
              "      <td>0.604813</td>\n",
              "      <td>0.496884</td>\n",
              "      <td>0.14114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>290.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>6.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.34000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>125.750000</td>\n",
              "      <td>308.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>8.127500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.63000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>250.500000</td>\n",
              "      <td>317.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.50000</td>\n",
              "      <td>8.560000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.72000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>375.250000</td>\n",
              "      <td>325.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>9.040000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.82000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>500.000000</td>\n",
              "      <td>340.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.00000</td>\n",
              "      <td>9.920000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.97000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4253f94-f1d4-4a1d-9b66-3a1250b9716a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e4253f94-f1d4-4a1d-9b66-3a1250b9716a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e4253f94-f1d4-4a1d-9b66-3a1250b9716a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow\timport keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# load admissions data\n",
        "#admissions_data = pd.read_csv(\"admissions_data.csv\")\n",
        "#print(admissions_data.head())\n",
        "\n",
        "#admissions_data.describe()\n",
        "#print(admissions_data.shape)\n",
        "\n",
        "# mark predicted values\n",
        "labels = admissions_data.iloc[:,-1]\n",
        "#print(labels.describe())\n",
        "\n",
        "# mark features\n",
        "features = admissions_data.iloc[:, 1:8]\n",
        "\n",
        "#split our training and test set\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.25, random_state = 42)\n",
        "\n",
        "# standardizing our data by scaling it\n",
        "sc = StandardScaler()\n",
        "features_train_scale = sc.fit_transform(features_train)\n",
        "features_test_scale = sc.transform(features_test)\n",
        "\n",
        "# commented out code for if you want to check out the scaled data\n",
        "\n",
        "#features_train_scale = pd.DataFrame(features_train_scale, columns = features_train.columns)\n",
        "#features_test_scale = pd.DataFrame(features_test_scale, columns = features_test.columns)\n",
        "\n",
        "#print(features_train_scale.describe())\n",
        "#print(features_test_scale.describe())\n",
        "\n",
        "# function to design the model\n",
        "def design_model(feature_data):\n",
        "\tmodel = Sequential()\n",
        "\tnum_features = feature_data.shape[1]\n",
        "\tinput = tf.keras.Input(shape=(num_features))\n",
        "\tmodel.add(input)\n",
        "\t# this model has two hidden layers and two dropout layers\n",
        "\t# relu activation function is used at both hidden layers\n",
        "\thidden_layer = layers.Dense(16, activation='relu')\n",
        "\tmodel.add(hidden_layer)\n",
        "\tmodel.add(layers.Dropout(0.1))\n",
        "\thidden_layer_2 = layers.Dense(8, activation='relu')\n",
        "\tmodel.add(hidden_layer_2)\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\tmodel.add(layers.Dense(1))\n",
        "\n",
        "\t# using an adam optimizer with a learning rate of 0.005\n",
        "\t# using mean-squared error as our loss function and mean average error as our metric\n",
        "\topt = keras.optimizers.Adam(learning_rate=0.005)\n",
        "\tmodel.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# apply the model to the scaled training data\n",
        "model = design_model(features_train_scale)\n",
        "#print(model.summary())\n",
        "\n",
        "# apply early stopping for efficiency\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "\n",
        "# fit the model with 100 epochs and a batch size of 8\n",
        "# validation split at 0.25\n",
        "history = model.fit(features_train_scale, labels_train.to_numpy(), epochs=100, batch_size=8, verbose=1, validation_split=0.25, callbacks=[es])\n",
        "\n",
        "# evaluate the model\n",
        "val_mse, val_mae = model.evaluate(features_test_scale, labels_test.to_numpy(), verbose = 0)\n",
        "\n",
        "# view the MAE performance\n",
        "print(\"MAE: \", val_mae)\n",
        "\n",
        "# evauate r-squared score\n",
        "y_pred = model.predict(features_test_scale)\n",
        "\n",
        "print(r2_score(labels_test,y_pred))\n",
        "\n",
        "# plot MAE and val_MAE over each epoch\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(2, 1, 1)\n",
        "ax1.plot(history.history['mae'])\n",
        "ax1.plot(history.history['val_mae'])\n",
        "ax1.set_title('model mae')\n",
        "ax1.set_ylabel('MAE')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "# Plot loss and val_loss over each epoch\n",
        "ax2 = fig.add_subplot(2, 1, 2)\n",
        "ax2.plot(history.history['loss'])\n",
        "ax2.plot(history.history['val_loss'])\n",
        "ax2.set_title('model loss')\n",
        "ax2.set_ylabel('loss')\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QeVr_xeVDkY-",
        "outputId": "9e147a67-23db-48de-a922-c1d3bc526ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - 1s 5ms/step - loss: 0.2921 - mae: 0.4410 - val_loss: 0.0771 - val_mae: 0.2300\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0899 - mae: 0.2439 - val_loss: 0.0394 - val_mae: 0.1770\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0651 - mae: 0.2016 - val_loss: 0.0204 - val_mae: 0.1232\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0404 - mae: 0.1610 - val_loss: 0.0173 - val_mae: 0.1128\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0313 - mae: 0.1403 - val_loss: 0.0183 - val_mae: 0.1165\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0284 - mae: 0.1331 - val_loss: 0.0108 - val_mae: 0.0891\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0205 - mae: 0.1136 - val_loss: 0.0095 - val_mae: 0.0833\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0960 - val_loss: 0.0086 - val_mae: 0.0788\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0160 - mae: 0.0981 - val_loss: 0.0065 - val_mae: 0.0657\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0964 - val_loss: 0.0083 - val_mae: 0.0771\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0809 - val_loss: 0.0067 - val_mae: 0.0697\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0131 - mae: 0.0887 - val_loss: 0.0064 - val_mae: 0.0669\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0829 - val_loss: 0.0070 - val_mae: 0.0694\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0772 - val_loss: 0.0063 - val_mae: 0.0652\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0099 - mae: 0.0778 - val_loss: 0.0055 - val_mae: 0.0574\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0758 - val_loss: 0.0063 - val_mae: 0.0658\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0712 - val_loss: 0.0061 - val_mae: 0.0641\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0095 - mae: 0.0756 - val_loss: 0.0071 - val_mae: 0.0709\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0108 - mae: 0.0805 - val_loss: 0.0070 - val_mae: 0.0691\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0093 - mae: 0.0744 - val_loss: 0.0076 - val_mae: 0.0728\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0093 - mae: 0.0770 - val_loss: 0.0060 - val_mae: 0.0632\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0680 - val_loss: 0.0061 - val_mae: 0.0635\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0083 - mae: 0.0692 - val_loss: 0.0054 - val_mae: 0.0575\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0697 - val_loss: 0.0064 - val_mae: 0.0662\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0087 - mae: 0.0717 - val_loss: 0.0056 - val_mae: 0.0607\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0682 - val_loss: 0.0049 - val_mae: 0.0552\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0073 - mae: 0.0661 - val_loss: 0.0049 - val_mae: 0.0551\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0683 - val_loss: 0.0046 - val_mae: 0.0529\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0649 - val_loss: 0.0048 - val_mae: 0.0557\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0650 - val_loss: 0.0043 - val_mae: 0.0495\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0665 - val_loss: 0.0048 - val_mae: 0.0539\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0707 - val_loss: 0.0045 - val_mae: 0.0515\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0629 - val_loss: 0.0050 - val_mae: 0.0560\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0618 - val_loss: 0.0045 - val_mae: 0.0512\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0617 - val_loss: 0.0042 - val_mae: 0.0493\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0601 - val_loss: 0.0042 - val_mae: 0.0516\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0671 - val_loss: 0.0042 - val_mae: 0.0490\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0665 - val_loss: 0.0044 - val_mae: 0.0522\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0679 - val_loss: 0.0049 - val_mae: 0.0577\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0637 - val_loss: 0.0043 - val_mae: 0.0514\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0639 - val_loss: 0.0054 - val_mae: 0.0621\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0647 - val_loss: 0.0041 - val_mae: 0.0474\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0640 - val_loss: 0.0043 - val_mae: 0.0494\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0064 - mae: 0.0613 - val_loss: 0.0044 - val_mae: 0.0535\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0066 - mae: 0.0642 - val_loss: 0.0040 - val_mae: 0.0488\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0066 - mae: 0.0621 - val_loss: 0.0046 - val_mae: 0.0541\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0603 - val_loss: 0.0035 - val_mae: 0.0434\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0601 - val_loss: 0.0040 - val_mae: 0.0467\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0611 - val_loss: 0.0039 - val_mae: 0.0461\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0649 - val_loss: 0.0046 - val_mae: 0.0531\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0616 - val_loss: 0.0039 - val_mae: 0.0481\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - mae: 0.0581 - val_loss: 0.0040 - val_mae: 0.0452\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0596 - val_loss: 0.0037 - val_mae: 0.0461\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0606 - val_loss: 0.0043 - val_mae: 0.0517\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0066 - mae: 0.0610 - val_loss: 0.0041 - val_mae: 0.0508\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0591 - val_loss: 0.0039 - val_mae: 0.0488\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0585 - val_loss: 0.0038 - val_mae: 0.0484\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0601 - val_loss: 0.0046 - val_mae: 0.0567\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0575 - val_loss: 0.0039 - val_mae: 0.0491\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0578 - val_loss: 0.0034 - val_mae: 0.0443\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0602 - val_loss: 0.0034 - val_mae: 0.0428\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0582 - val_loss: 0.0035 - val_mae: 0.0449\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0599 - val_loss: 0.0036 - val_mae: 0.0462\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0587 - val_loss: 0.0036 - val_mae: 0.0471\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0637 - val_loss: 0.0044 - val_mae: 0.0543\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0597 - val_loss: 0.0035 - val_mae: 0.0455\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0051 - mae: 0.0548 - val_loss: 0.0034 - val_mae: 0.0458\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0578 - val_loss: 0.0030 - val_mae: 0.0407\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0629 - val_loss: 0.0038 - val_mae: 0.0501\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0631 - val_loss: 0.0035 - val_mae: 0.0460\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0595 - val_loss: 0.0032 - val_mae: 0.0424\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0058 - mae: 0.0580 - val_loss: 0.0037 - val_mae: 0.0468\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0600 - val_loss: 0.0033 - val_mae: 0.0414\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0054 - mae: 0.0568 - val_loss: 0.0034 - val_mae: 0.0462\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0579 - val_loss: 0.0034 - val_mae: 0.0435\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0585 - val_loss: 0.0036 - val_mae: 0.0477\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0569 - val_loss: 0.0043 - val_mae: 0.0546\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0572 - val_loss: 0.0035 - val_mae: 0.0453\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0594 - val_loss: 0.0032 - val_mae: 0.0424\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0580 - val_loss: 0.0035 - val_mae: 0.0439\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0586 - val_loss: 0.0036 - val_mae: 0.0470\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0539 - val_loss: 0.0036 - val_mae: 0.0459\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0051 - mae: 0.0520 - val_loss: 0.0039 - val_mae: 0.0499\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0525 - val_loss: 0.0036 - val_mae: 0.0482\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0582 - val_loss: 0.0033 - val_mae: 0.0424\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0547 - val_loss: 0.0035 - val_mae: 0.0459\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0552 - val_loss: 0.0038 - val_mae: 0.0455\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0602 - val_loss: 0.0034 - val_mae: 0.0440\n",
            "Epoch 88: early stopping\n",
            "MAE:  0.0457846000790596\n",
            "0.8005581160198613\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}